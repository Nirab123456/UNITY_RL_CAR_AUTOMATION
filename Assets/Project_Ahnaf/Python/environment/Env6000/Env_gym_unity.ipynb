{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaceful_pie.unity_comms import UnityComms\n",
    "import argparse\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, MultiBinary,Discrete\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import FrameStack\n",
    "from collections import deque\n",
    "# Import UnityComms from peaceful_pie.unity_comms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie import ray_results_helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unity_comms(port: int):\n",
    "    unity_comms = UnityComms(port)\n",
    "    return unity_comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 6000  # Replace with your desired port number\n",
    "unity_comms_instance = unity_comms(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_comms_instance \n",
    "unity_comms = unity_comms_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVector3:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityEnv(Env):\n",
    "    def __init__(self, unity_comms, i):\n",
    "        self.unity_comms = unity_comms\n",
    "        self.action_space = Discrete(15)  # Scale the action range to -1.0 to 1.0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        self.initial_position = None\n",
    "        self.prev_position = None\n",
    "        self.prev_velocity = 0\n",
    "        self.initial_get_CarCollisionDetected =0\n",
    "        self.frame_count=0\n",
    "        self.i = i\n",
    "\n",
    "    def RayCast(self):\n",
    "        ray_results = getattr(self.unity_comms, f\"GetRayCastsResults_{self.i}\")()\n",
    "        distance = np.array(ray_results['rayDistances'], dtype=np.float32)\n",
    "        types = np.array(ray_results['rayHitObjectTypes'], dtype=np.int32)\n",
    "        num_types = ray_results['NumObjectTypes']\n",
    "        self.actual_result = ray_results_helper.ray_results_to_feature_np(\n",
    "            ray_results_helper.RayResults(\n",
    "                NumObjectTypes=num_types,\n",
    "                rayDistances=distance,\n",
    "                rayHitObjectTypes=types,\n",
    "            )\n",
    "        )  \n",
    "\n",
    "        return self.actual_result\n",
    "    \n",
    "\n",
    "    def Is_obstacle_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        obstacle_channel = actual_result[0]  # Extract the obstacle channel from the actual result\n",
    "        if np.max(obstacle_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def Is_reward_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        reward_channel = actual_result[1]  # Extract the reward channel from the actual result\n",
    "        if np.max(reward_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def Goal(self):\n",
    "        if self.Is_reward_visible() == True:\n",
    "            goal_channel = self.actual_result[1]\n",
    "            if np.max(goal_channel) > 1:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def Get_reward(self):\n",
    "        obstacle_penalty = -1.0  # Penalty for encountering an obstacle\n",
    "        reward_bonus = .7 # Reward for finding a reward object\n",
    "        goal_reward = 1  # Reward for reaching the goal\n",
    "        car_collision_penalty = -1.0  # Penalty for colliding with the car\n",
    "        valocity_reward = 0.1 #reward for moving forward\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.Is_obstacle_visible()== True:\n",
    "            reward += obstacle_penalty\n",
    "\n",
    "        if self.Is_reward_visible()== True:\n",
    "            reward += reward_bonus\n",
    "\n",
    "        reward += self.Step_panalty()\n",
    "\n",
    "        if self.Goal() == True:\n",
    "            reward += goal_reward\n",
    "\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            reward += car_collision_penalty\n",
    "\n",
    "        if self.Check_valocity_increment()== True:\n",
    "            reward += valocity_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.RayCast()\n",
    "        Reset_Car = getattr(self.unity_comms, f\"ResetPosition_{self.i}\")()  \n",
    "        Reset_Agent = getattr(self.unity_comms, f\"ResetPosition_Plane_{self.i}\")()\n",
    "        position =self.Get_position()\n",
    "        position = np.array(position, dtype=np.float32)\n",
    "        self.frame_count=0\n",
    "        return position\n",
    "\n",
    "    def done(self):\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_movingPlaneCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_planeCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Check_stuck() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        else:\n",
    "            self.frame_count += 1\n",
    "            return False\n",
    "\n",
    "    def Get_carCollisionDetected(self):\n",
    "        collision_count = 0\n",
    "        collision = collision = getattr(self.unity_comms, f\"CarCollisionDetected_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >=2:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_movingPlaneCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetMovingPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >= 1:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_planeCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count > 0:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def Get_rewardCollision(self):\n",
    "        collision = getattr(self.unity_comms, f\"GetRewardCollision_{self.i}\")()\n",
    "        self.rewardcollision = collision\n",
    "\n",
    "    def Step_panalty(self):\n",
    "        if self.done() == True:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action based on the provided action index\n",
    "        if action == 0:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "        elif action == 1:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "        elif action == 2:\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 3:\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 4:\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 5:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 6:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 7:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 8:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 9:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 10:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 11:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 12:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 13:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 14:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action index\")\n",
    "\n",
    "        self.prev_position = self.Get_position()\n",
    "        self.prev_velocity = self.Get_velocity()\n",
    "\n",
    "        position = self.Get_position()\n",
    "        velocity = self.Get_velocity()\n",
    "        # Concatenate the position and velocity to form the observation\n",
    "        observation = np.array([position[0], position[1], velocity], dtype=np.float32)\n",
    "\n",
    "        reward = self.Get_reward()\n",
    "        done = self.done()\n",
    "    # Update the info dictionary with relevant information\n",
    "        info = {\n",
    "            'episode_reward': reward,  # Replace with the actual episode reward\n",
    "            'episode_length': self.frame_count,  # Replace with the actual episode length\n",
    "            'current_observation': observation,\n",
    "            'action_taken': action,\n",
    "            'obstacle_visible': self.Is_obstacle_visible(),\n",
    "            'reward_visible': self.Is_reward_visible(),\n",
    "            'goal_reached': self.Goal(),\n",
    "            'car_collision_detected': self.Get_carCollisionDetected(),\n",
    "            'velocity_incremented': self.Check_valocity_increment(),\n",
    "        }\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "    def Get_velocity(self):\n",
    "        valocity =  getattr(self.unity_comms, f\"CarSpeedUI_{self.i}\")()\n",
    "        return valocity\n",
    "\n",
    "    def Check_valocity_increment(self):\n",
    "        valocity = self.Get_velocity()\n",
    "        if valocity > self.prev_velocity:\n",
    "            self.prev_velocity = valocity\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def Check_stuck(self):\n",
    "        position_threshold = 0.01\n",
    "        consecutive_steps = 20\n",
    "        position_counter = 0\n",
    "        \n",
    "        if self.prev_position is None:\n",
    "            self.prev_position = self.get_position()\n",
    "\n",
    "        for _ in range(consecutive_steps):\n",
    "            x, y, z = self.Get_position()\n",
    "\n",
    "            position_diff = np.linalg.norm(np.array([x, y, z]) - np.array(self.prev_position))\n",
    "            if position_diff < position_threshold:\n",
    "                position_counter += 1\n",
    "            else:\n",
    "                position_counter = 0\n",
    "\n",
    "            self.prev_position = [x, y, z]\n",
    "\n",
    "        if position_counter >= consecutive_steps:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def Get_position(self):\n",
    "        position = getattr(self.unity_comms, f\"GetPosition_{self.i}\")()\n",
    "        # Extract x, y, and z components from position\n",
    "        x = position['x']\n",
    "        y = position['y']\n",
    "        z = position['z']\n",
    "        return x, y, z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frame skip frequency\n",
    "frame_skip_frequency = 5\n",
    "\n",
    "# Define the number of training steps\n",
    "total_timesteps = 100000\n",
    "\n",
    "# Define the directory paths\n",
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(OPT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optuna for HPO\n",
    "import optuna\n",
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Evaluate Policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import wrappers\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import os\n",
    "LOG_DIR = './logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://github.com/araffin/rl-baselines-zoo/issues/29\n",
    "def optimize_ppo(trial):\n",
    "    \"\"\" Learning hyperparamters we want to optimise\"\"\"\n",
    "    return {\n",
    "        'n_steps': trial.suggest_int('n_steps', 2048, 20480),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 0.8, 0.9999),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-4),\n",
    "        'clip_range': trial.suggest_uniform('clip_range', 0.1, 0.4),\n",
    "        'gae_lambda': trial.suggest_uniform('gae_lambda', 0.8, .99)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def optimize_agent(trial):\n",
    "    try:\n",
    "        model_params = optimize_ppo(trial)\n",
    "        env = UnityEnv(unity_comms,6000)\n",
    "        env = Monitor(env, LOG_DIR)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        model = PPO('MlpPolicy', env,batch_size=1024, tensorboard_log=LOG_DIR, verbose=0, **model_params)\n",
    "        model.learn(total_timesteps=20480)\n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        return mean_reward\n",
    "    except Exception as e: \n",
    "        return -1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimize_agent, n_trials=5, n_jobs=1)\n",
    "model_params = study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "env = UnityEnv(unity_comms,6000)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = PPO('MlpPolicy', env,batch_size=4096, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
