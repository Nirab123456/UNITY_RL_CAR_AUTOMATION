{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peaceful_pie.unity_comms import UnityComms\n",
    "import argparse\n",
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Box, MultiBinary,Discrete\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.wrappers import FrameStack\n",
    "from collections import deque\n",
    "# Import UnityComms from peaceful_pie.unity_comms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie.unity_comms import UnityComms\n",
    "from peaceful_pie import ray_results_helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unity_comms(port: int):\n",
    "    unity_comms = UnityComms(port)\n",
    "    return unity_comms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 9000  # Replace with your desired port number\n",
    "unity_comms_instance = unity_comms(port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unity_comms_instance \n",
    "unity_comms = unity_comms_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVector3:\n",
    "    def __init__(self, x, y, z):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnityEnv(Env):\n",
    "    def __init__(self, unity_comms, i):\n",
    "        self.unity_comms = unity_comms\n",
    "        self.action_space = Discrete(15)  # Scale the action range to -1.0 to 1.0\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)\n",
    "        self.initial_position = None\n",
    "        self.prev_position = None\n",
    "        self.prev_velocity = 0\n",
    "        self.initial_get_CarCollisionDetected =0\n",
    "        self.frame_count=0\n",
    "        self.i = i\n",
    "\n",
    "    def RayCast(self):\n",
    "        ray_results = getattr(self.unity_comms, f\"GetRayCastsResults_{self.i}\")()\n",
    "        distance = np.array(ray_results['rayDistances'], dtype=np.float32)\n",
    "        types = np.array(ray_results['rayHitObjectTypes'], dtype=np.int32)\n",
    "        num_types = ray_results['NumObjectTypes']\n",
    "        self.actual_result = ray_results_helper.ray_results_to_feature_np(\n",
    "            ray_results_helper.RayResults(\n",
    "                NumObjectTypes=num_types,\n",
    "                rayDistances=distance,\n",
    "                rayHitObjectTypes=types,\n",
    "            )\n",
    "        )  \n",
    "\n",
    "        return self.actual_result \n",
    "    \n",
    "\n",
    "    def Is_obstacle_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        self.obstacle_channel = actual_result[0]  # Extract the obstacle channel from the actual result\n",
    "        if np.max(self.obstacle_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def Is_obstacle_Collision_detected(self):\n",
    "        if self.Is_obstacle_visible()== True:\n",
    "            actual_result=self.RayCast()\n",
    "            self.obstacle_channel = actual_result[0]\n",
    "            if np.max(self.obstacle_channel) >=1:\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "    def Is_reward_visible(self):\n",
    "        actual_result=self.RayCast()\n",
    "        reward_channel = actual_result[1]  # Extract the reward channel from the actual result\n",
    "        if np.max(reward_channel) > 0:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def Goal(self):\n",
    "        if self.Is_reward_visible() == True:\n",
    "            goal_channel = self.actual_result[1]\n",
    "            if np.max(goal_channel) > 1:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def Get_reward(self):\n",
    "        obstacle_penalty = -0.25  # Penalty for encountering an obstacle\n",
    "        reward_bonus = .7 # Reward for finding a reward object\n",
    "        goal_reward = 1  # Reward for reaching the goal\n",
    "        car_collision_penalty = -1.0  # Penalty for colliding with the car\n",
    "        valocity_reward = 0.1 #reward for moving forward\n",
    "        obstacle_collision_panalty = -.5 #penalty for colliding with obstacle\n",
    "\n",
    "        reward = 0.0\n",
    "\n",
    "        if self.Is_obstacle_visible()== True:\n",
    "            reward += obstacle_penalty\n",
    "\n",
    "        if self.Is_reward_visible()== True:\n",
    "            reward += reward_bonus\n",
    "\n",
    "        reward += self.Step_panalty()\n",
    "\n",
    "        if self.Goal() == True:\n",
    "            reward += goal_reward\n",
    "\n",
    "        if self.Is_obstacle_Collision_detected()==True:\n",
    "            reward +=obstacle_collision_panalty\n",
    "\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            reward += car_collision_penalty\n",
    "\n",
    "        if self.Check_valocity_increment()== True:\n",
    "            reward += valocity_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.RayCast()\n",
    "        Reset_Car = getattr(self.unity_comms, f\"ResetPosition_{self.i}\")()  \n",
    "        Reset_Agent = getattr(self.unity_comms, f\"ResetPosition_Plane_{self.i}\")()\n",
    "        position =self.Get_position()\n",
    "        position = np.array(position, dtype=np.float32)\n",
    "        self.frame_count=0\n",
    "        return position\n",
    "\n",
    "    def done(self):\n",
    "        if self.Get_carCollisionDetected() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_movingPlaneCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Get_planeCollision() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        elif self.Check_stuck() == True:\n",
    "            self.frame_count=0\n",
    "            return True\n",
    "        else:\n",
    "            self.frame_count += 1\n",
    "            return False\n",
    "\n",
    "    def Get_carCollisionDetected(self):\n",
    "        collision_count = 0\n",
    "        collision = collision = getattr(self.unity_comms, f\"CarCollisionDetected_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >=2:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_movingPlaneCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetMovingPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count >= 1:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def Get_planeCollision(self):\n",
    "        collision_count = 0\n",
    "        collision = getattr(self.unity_comms, f\"GetPlaneCollision_{self.i}\")()\n",
    "        if collision == 1:\n",
    "            collision_count += 1\n",
    "            if collision_count > 0:\n",
    "                collision_count = 0\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    def Get_rewardCollision(self):\n",
    "        collision = getattr(self.unity_comms, f\"GetRewardCollision_{self.i}\")()\n",
    "        self.rewardcollision = collision\n",
    "\n",
    "    def Step_panalty(self):\n",
    "        if self.done() == True:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        # Perform the action based on the provided action index\n",
    "        if action == 0:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "        elif action == 1:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "        elif action == 2:\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 3:\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 4:\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 5:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 6:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 7:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 8:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "        elif action == 9:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "        elif action == 10:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 11:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 12:\n",
    "            getattr(self.unity_comms, f\"GoForward_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 13:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnLeft_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        elif action == 14:\n",
    "            getattr(self.unity_comms, f\"GoReverse_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"TurnRight_{self.i}\")()\n",
    "            getattr(self.unity_comms, f\"Handbrake_{self.i}\")()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action index\")\n",
    "\n",
    "        self.prev_position = self.Get_position()\n",
    "        self.prev_velocity = self.Get_velocity()\n",
    "\n",
    "        position = self.Get_position()\n",
    "        velocity = self.Get_velocity()\n",
    "        # Concatenate the position and velocity to form the observation\n",
    "        observation = np.array([position[0], position[1], velocity], dtype=np.float32)\n",
    "\n",
    "        reward = self.Get_reward()\n",
    "        done = self.done()\n",
    "    # Update the info dictionary with relevant information\n",
    "        info = {\n",
    "            'episode_reward': reward,  # Replace with the actual episode reward\n",
    "            'episode_length': self.frame_count,  # Replace with the actual episode length\n",
    "            'current_observation': observation,\n",
    "            'action_taken': action,\n",
    "            'obstacle_visible': self.Is_obstacle_visible(),\n",
    "            'reward_visible': self.Is_reward_visible(),\n",
    "            'goal_reached': self.Goal(),\n",
    "            'car_collision_detected': self.Get_carCollisionDetected(),\n",
    "            'velocity_incremented': self.Check_valocity_increment(),\n",
    "        }\n",
    "        return observation, reward, done, info\n",
    "\n",
    "\n",
    "    def Get_velocity(self):\n",
    "        valocity =  getattr(self.unity_comms, f\"CarSpeedUI_{self.i}\")()\n",
    "        return valocity\n",
    "\n",
    "    def Check_valocity_increment(self):\n",
    "        valocity = self.Get_velocity()\n",
    "        if valocity > self.prev_velocity:\n",
    "            self.prev_velocity = valocity\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def Check_stuck(self):\n",
    "        position_threshold = 0.01\n",
    "        consecutive_steps = 20\n",
    "        position_counter = 0\n",
    "        \n",
    "        if self.prev_position is None:\n",
    "            self.prev_position = self.get_position()\n",
    "\n",
    "        for _ in range(consecutive_steps):\n",
    "            x, y, z = self.Get_position()\n",
    "\n",
    "            position_diff = np.linalg.norm(np.array([x, y, z]) - np.array(self.prev_position))\n",
    "            if position_diff < position_threshold:\n",
    "                position_counter += 1\n",
    "            else:\n",
    "                position_counter = 0\n",
    "\n",
    "            self.prev_position = [x, y, z]\n",
    "\n",
    "        if position_counter >= consecutive_steps:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def Get_position(self):\n",
    "        position = getattr(self.unity_comms, f\"GetPosition_{self.i}\")()\n",
    "        # Extract x, y, and z components from position\n",
    "        x = position['x']\n",
    "        y = position['y']\n",
    "        z = position['z']\n",
    "        return x, y, z\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raycast(UnityEnv):\n",
    "    def __init__(self, unity_comms, i):\n",
    "        super().__init__(unity_comms, i)\n",
    "\n",
    "    def Full_ray_analysis(self):\n",
    "        ray_results = getattr(self.unity_comms, f\"GetRayCastsResults_{self.i}\")()\n",
    "        distance = np.array(ray_results['rayDistances'], dtype=np.float32)\n",
    "        types = np.array(ray_results['rayHitObjectTypes'], dtype=np.int32)\n",
    "        num_types = ray_results['NumObjectTypes']\n",
    "        self.actual_result = ray_results_helper.ray_results_to_feature_np(\n",
    "            ray_results_helper.RayResults(\n",
    "                NumObjectTypes=num_types,\n",
    "                rayDistances=distance,\n",
    "                rayHitObjectTypes=types,\n",
    "            )\n",
    "        )  \n",
    "\n",
    "        return ray_results,distance,types,num_types,self.actual_result \n",
    "    \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_analysis = Raycast(unity_comms, 9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the frame skip frequency\n",
    "frame_skip_frequency = 5\n",
    "\n",
    "# Define the number of training steps\n",
    "total_timesteps = 100000\n",
    "\n",
    "# Define the directory paths\n",
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(OPT_DIR, exist_ok=True)\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import optuna for HPO\n",
    "import optuna\n",
    "# Import PPO for algos\n",
    "from stable_baselines3 import PPO\n",
    "# Evaluate Policy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "# Import wrappers\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "import os\n",
    "LOG_DIR = './logs/'\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "OPT_DIR = './opt_modeldata/'\n",
    "if not os.path.exists(OPT_DIR):\n",
    "    os.makedirs(OPT_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rifat\\miniconda3\\envs\\tf\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:140: UserWarning: You have specified a mini-batch size of 4096, but because the `RolloutBuffer` is of size `n_steps * n_envs = 4885`, after every 1 untruncated mini-batches, there will be a truncated mini-batch of size 789\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=4885 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/PPO_5\n",
      "requests.exceptions.ConnectionError => ignoring, retrying\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 4.48     |\n",
      "|    ep_rew_mean     | 0.951    |\n",
      "| time/              |          |\n",
      "|    fps             | 0        |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6532     |\n",
      "|    total_timesteps | 4885     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.82         |\n",
      "|    ep_rew_mean          | -1.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 13261        |\n",
      "|    total_timesteps      | 9770         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.099606e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | -0.132       |\n",
      "|    learning_rate        | 4.46e-05     |\n",
      "|    loss                 | 0.637        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000563    |\n",
      "|    value_loss           | 1.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.51         |\n",
      "|    ep_rew_mean          | -0.47        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 20220        |\n",
      "|    total_timesteps      | 14655        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.422804e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | -0.125       |\n",
      "|    learning_rate        | 4.46e-05     |\n",
      "|    loss                 | 0.535        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00025     |\n",
      "|    value_loss           | 1.17         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.07         |\n",
      "|    ep_rew_mean          | -0.214       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 0            |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 26802        |\n",
      "|    total_timesteps      | 19540        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.459097e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -2.71        |\n",
      "|    explained_variance   | -0.693       |\n",
      "|    learning_rate        | 4.46e-05     |\n",
      "|    loss                 | 0.243        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000498    |\n",
      "|    value_loss           | 0.528        |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Import os for file path management\n",
    "import os \n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True\n",
    "    \n",
    "CHECKPOINT_DIR = './train_modeldata/'\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "env = UnityEnv(unity_comms,8000)\n",
    "env = Monitor(env, LOG_DIR)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "\n",
    "\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "model_params = {\n",
    "    'n_steps': 4885,\n",
    "    'gamma': 0.839729258597705,\n",
    "    'learning_rate': 4.456287316411457e-05,\n",
    "    'clip_range': 0.29999077694725984,\n",
    "    'gae_lambda': 0.9074598206209127\n",
    "}\n",
    "\n",
    "model = PPO('MlpPolicy', env, batch_size=4096, tensorboard_log=LOG_DIR, verbose=1, **model_params)\n",
    "\n",
    "\n",
    "model.load(\"D:/RL_UNITY/new_env_car_multicar_developer_bug_fixed/Assets/subAssets/Python/environment/Env8000/train_modeldata/best_model_10000.zip\")\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
